# -*- coding: utf-8 -*-
"""DT_LLM_appendix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xKKX3FkwLLy2PbvWcgqp5R_VTx6LcrhH
"""

#1. Install the Tesseract engine
!apt-get update -qq
!apt-get install -y -qq tesseract-ocr libtesseract-dev  #ocr - text from image
!apt-get update -qq
!apt-get install -y poppler-utils #extracts pages from pdf
!pip install PyPDF2
!pip install pdf2image #extracts pages from pdf
# 2. Install the Python packages
!pip install --quiet pytesseract pillow
from IPython import get_ipython
from IPython.display import display
# %%
# ‚úÖ Combined Whole-Text and Chunk-Based Deductive Coding
# Google Colab-ready | GPT-4o | Secure API | Codebook x PDF

from google.colab import drive
import pandas as pd
import PyPDF2
import time
from getpass import getpass
from openai import OpenAI
import pytesseract
from pdf2image import convert_from_path
import os
import re

# === Step 1: Mount Google Drive, Establish I/O Directories ===
drive.mount('/content/drive')

#INPUT REQUIRED
#Pathing -- CHANGE THE PATHING TO MATCH USER DRIVE
codebook_path = "/content/drive/MyDrive/" #for codebook in .csv format
output_path = "/content/drive/MyDrive/"

#INPUT REQUIRED
#OpenAI Model -- match https://platform.openai.com/docs/models
MODEL_NAME = "gpt-4o-mini"

#INPUT REQUIRED
#update csv encoding if needed
codebook_df = pd.read_csv(codebook_path, encoding="latin1")
codebook_df.columns = ["flag", "description", "example"]
print(codebook_df.head())

#=== Step 2: Define Functions for LLM Call and Text Chunking ===
# Step 2.1: Securely Authenticate GPT
api_key = getpass("üîê Enter your OpenAI API key: ")
client = OpenAI(api_key=api_key)


# Step 2.2: Prompt Generator
def generate_prompt_ss(flag, description, report_text, example):
    #print("üìù Prompt length:", len(report_text), "characters")
    return (
f"Explain whether the parameter '{flag}' is mentioned/directly talked about in the following text and provide evidence from the text."
f"If it does, briefly explain how (3-5 sentences with ~2 pieces of evidence); if it does not match, briefly explain why the paper does not focus on it (1 sentence)."
f"Note that '{flag}' is defined as '{description}'. An example sentence would be: '{example}'.\n"
f"{report_text}"
    )

# Step 2.3: Chunk Generation
def split_into_chunks(text, words_per_chunk=500):
    words = text.split()
    return [' '.join(words[i:i+words_per_chunk]) for i in range(0, len(words), words_per_chunk)]

#Step 2.4: Text Cleaning
def clean_ocr_text(raw_text):
            # Remove erroneous newline characters and unwanted symbols\n",
            cleaned_text = re.sub(r'\\n', ' ', raw_text)  # Replace newlines with space\n",
            cleaned_text = re.sub(r'[@‚Äú‚Äù\\|]', '', cleaned_text)  # Remove specific unwanted characters\n",
            cleaned_text = re.sub(r' ‚Äì ', '-', cleaned_text)  # Correct spaced hyphens\n",
             # Replace incorrect newlines (e.g., those within words)\n",
            cleaned_text = re.sub(r'(?<=[a-zA-Z])-(?=[a-zA-Z])', '', cleaned_text)            # Remove any other post-processing steps you need here\n",
            return cleaned_text

#=== STEP 3: Pull all files in folder ===
folderPath = "/content/drive/My Drive/"
fileLst = os.listdir(folderPath) #list of files to loop over
print(fileLst)
print('Loading Done.')

#=== Step 4: Use OCR to pull text ===
#=== CHUNK APPROACH ===
for file in fileLst:
  #generate pages
  fileFullPath = os.path.join(folderPath,file)
  print(fileFullPath)
  fullText = '' #initialize text string to be chunked later

  #Check that syntax is right. Otherwise Break.
  if not(os.path.exists(os.path.join(folderPath,file))):
    print(file + " -- failure")
    break
  else:

    #Extract pages
    pages = convert_from_path(fileFullPath)
    print(file + ": " + str(len(pages)) + " Pages")

    #loop over pages to generate strings
    for page in pages:

      text = pytesseract.image_to_string(page)
      fullText += text #concatenate
    fullText = clean_ocr_text(fullText)

    print(file +" Read.\n Beginning GPT Prompting...")

     #=== STEP 6: Chunk-Based Text Analysis ===

    chunks = split_into_chunks(fullText, 500)
    print(f"‚úÖ {file} split into {len(chunks)} chunks")

    iterations =16 #UPDATE THIS FOR 15 Iterations
    for iteration in range(9, 11): #loop over iterations
        results = []
        print("Chunk Iterations: " + str(iteration))

          #For each flag
        for _, row in codebook_df.iterrows():
            flag = row['flag']
            description = row['description']
            example = row['example']

            output = [] #initialize GPT response object
            print(flag)
            #Loop over the entire paper
            for chunk_id, chunk_text in enumerate(chunks):

              print("Chunk: " + str(chunk_id))

              prompt = generate_prompt_ss(flag, description, chunk_text,example)

              try:
               response = client.chat.completions.create(
               model=MODEL_NAME,
               messages=[{"role": "user", "content": prompt}],
               temperature=1.0
               )
               output.append(response.choices[0].message.content)
               time.sleep(1)
              except Exception as e:
                print(f"‚ö†Ô∏è Chunk error in chunk {chunk_id+1}, dimension {flag}: {e}")
            print(flag + ": Done")
            results.append({
                    "Iteration": iteration,
                    "Report": file.split("/")[-1],
                    "Model": MODEL_NAME,
                    "Prompt_type": "Chunked Text",
                    "Flag": flag,
                    "Output": output})
            #end chunk loop
          #end flag loop

        #save results once per iteration
        pd.DataFrame(results)[["Iteration", "Report", "Model", "Prompt_type", "Flag", "Output"]].to_csv(output_path, index=False, encoding="utf-8", mode = 'a')
        print(f"‚úÖ Results saved to: {output_path}")

        #end iteration loop
      #end file loop

#=== Step 5: Use OCR to pull text ===
#=== Whole Paper APPROACH ===
for file in fileLst:
  #generate pages
  fileFullPath = os.path.join(folderPath,file)
  print(fileFullPath)
  fullText = '' #initialize text string to be chunked later

  #Check that syntax is right. Otherwise Break.
  if not(os.path.exists(os.path.join(folderPath,file))):
    print(file + " -- failure")
    break
  else:

    #Extract pages
    pages = convert_from_path(fileFullPath)
    print(file + ": " + str(len(pages)) + " Pages")

    #loop over pages to generate strings
    for page in pages:

      text = pytesseract.image_to_string(page)
      fullText += text #concatenate
    fullText = clean_ocr_text(fullText)

    print(file +" Read.\n Beginning GPT Prompting...")

    iterations =16 #UPDATE THIS FOR 15 Iterations
    for iteration in range(1, iterations): #loop over iterations
        results = []
        print("Whole Paper Iterations: " + str(iteration))

          #For each flag
        for _, row in codebook_df.iterrows():
            flag = row['flag']
            description = row['description']
            example = row['example']

            output = [] #initialize GPT response object
            print(flag)
            prompt = generate_prompt_ss(flag, description, fullText,example)

            #Loop over the entire paper
            for modelname in ["gpt-4o", "gpt-4o-mini", "o1-mini"]:

              print("Model: " + str(modelname))
              try:
               response = client.chat.completions.create(
               model=modelname,
               messages=[{"role": "user", "content": prompt}],
               temperature=1.0
               )
               output.append(response.choices[0].message.content)
               time.sleep(1)

              except Exception as e:
                print(f"‚ö†Ô∏è Prompting error in model {modelname}, dimension {flag}: {e}")
            print(flag + ": Done")
            results.append({
                    "Iteration": iteration,
                    "Report": file.split("/")[-1],
                    "Model": modelname,
                    "Prompt_type": "Whole Text",
                    "Flag": flag,
                    "Output": output})
            #end model loop
          #end flag loop

        #save results once per iteration
        pd.DataFrame(results)[["Iteration", "Report", "Model", "Prompt_type", "Flag", "Output"]].to_csv(output_path, index=False, encoding="utf-8", mode = 'a')
        print(f"‚úÖ Results saved to: {output_path}")

        #end iteration loop
      #end file loop

#ARCHIVED
def read_pdf(file_path):
    with open(file_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        return "".join([page.extract_text() for page in reader.pages if page.extract_text()])

report_text = read_pdf(test_filename)